{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = \"TRUE\"\n",
    "os.environ['DASHSCOPE_API_KEY'] = \"xxxxxxxxxxx\"\n",
    "os.environ['MODELSCOPE_API_TOKEN'] = 'xxxxxxxxxxxx'"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5f5e1266bed5714f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "prompts = []\n",
    "outputs = []\n",
    "\n",
    "# 这个是python的猴子补丁(Monkey Patching) ，这里用于hook住modelscope_agent的代码，以便我们可以查看模型的input和output\n",
    "def fix_1():\n",
    "    import os\n",
    "    import random\n",
    "    import traceback\n",
    "    from http import HTTPStatus\n",
    "    from typing import Union\n",
    "\n",
    "    import dashscope\n",
    "    import json\n",
    "    from dashscope import Generation\n",
    "    from modelscope_agent.agent_types import AgentType\n",
    "\n",
    "    from modelscope_agent.llm.base import LLM\n",
    "    from modelscope_agent.llm.utils import DEFAULT_MESSAGE, CustomOutputWrapper\n",
    "\n",
    "    class my_DashScopeLLM(LLM):\n",
    "        name = 'dashscope_llm'\n",
    "\n",
    "        def __init__(self, cfg):\n",
    "            super().__init__(cfg)\n",
    "            self.model = self.cfg.get('model', 'modelscope-agent-llm-v1')\n",
    "            self.model_id = self.model\n",
    "            self.generate_cfg = self.cfg.get('generate_cfg', {})\n",
    "            self.agent_type = self.cfg.get('agent_type', AgentType.DEFAULT)\n",
    "\n",
    "        def generate(self,\n",
    "                     llm_artifacts: Union[str, dict],\n",
    "                     functions=[],\n",
    "                     **kwargs):\n",
    "            prompts.append(llm_artifacts)\n",
    "\n",
    "            # TODO retry and handle message\n",
    "            try:\n",
    "                if self.agent_type == AgentType.Messages:\n",
    "                    messages = llm_artifacts if len(\n",
    "                        llm_artifacts) > 0 else DEFAULT_MESSAGE\n",
    "                    self.generate_cfg['use_raw_prompt'] = False\n",
    "                    response = dashscope.Generation.call(\n",
    "                        model=self.model,\n",
    "                        messages=messages,\n",
    "                        # set the random seed, optional, default to 1234 if not set\n",
    "                        seed=random.randint(1, 10000),\n",
    "                        result_format=\n",
    "                        'message',  # set the result to be \"message\" format.\n",
    "                        stream=False,\n",
    "                        **self.generate_cfg)\n",
    "                    llm_result = CustomOutputWrapper.handle_message_chat_completion(\n",
    "                        response)\n",
    "                else:\n",
    "                    response = Generation.call(\n",
    "                        model=self.model,\n",
    "                        prompt=llm_artifacts,\n",
    "                        stream=False,\n",
    "                        **self.generate_cfg)\n",
    "                    llm_result = CustomOutputWrapper.handle_message_text_completion(\n",
    "                        response)\n",
    "                outputs.append(llm_result)\n",
    "                return llm_result\n",
    "            except Exception as e:\n",
    "                error = traceback.format_exc()\n",
    "                error_msg = f'LLM error with input {llm_artifacts} \\n dashscope error: {str(e)} with traceback {error}'\n",
    "                print(error_msg)\n",
    "                raise RuntimeError(error)\n",
    "\n",
    "            if self.agent_type == AgentType.MS_AGENT:\n",
    "                # in the form of text\n",
    "                idx = llm_result.find('<|endofthink|>')\n",
    "                if idx != -1:\n",
    "                    llm_result = llm_result[:idx + len('<|endofthink|>')]\n",
    "                return llm_result\n",
    "            elif self.agent_type == AgentType.Messages:\n",
    "                # in the form of message\n",
    "                return llm_result\n",
    "            else:\n",
    "                # in the form of text\n",
    "                return llm_result\n",
    "\n",
    "        def stream_generate(self,\n",
    "                            llm_artifacts: Union[str, dict],\n",
    "                            functions=[],\n",
    "                            **kwargs):\n",
    "            print('stream_generate')\n",
    "            prompts.append(llm_artifacts)\n",
    "\n",
    "            total_response = ''\n",
    "            try:\n",
    "                if self.agent_type == AgentType.Messages:\n",
    "                    self.generate_cfg['use_raw_prompt'] = False\n",
    "                    responses = Generation.call(\n",
    "                        model=self.model,\n",
    "                        messages=llm_artifacts,\n",
    "                        stream=True,\n",
    "                        result_format='message',\n",
    "                        **self.generate_cfg)\n",
    "                else:\n",
    "                    responses = Generation.call(\n",
    "                        model=self.model,\n",
    "                        prompt=llm_artifacts,\n",
    "                        stream=True,\n",
    "                        **self.generate_cfg)\n",
    "            except Exception as e:\n",
    "                error = traceback.format_exc()\n",
    "                error_msg = f'LLM error with input {llm_artifacts} \\n dashscope error: {str(e)} with traceback {error}'\n",
    "                print(error_msg)\n",
    "                raise RuntimeError(error)\n",
    "\n",
    "            for response in responses:\n",
    "                if response.status_code == HTTPStatus.OK:\n",
    "                    if self.agent_type == AgentType.Messages:\n",
    "                        llm_result = CustomOutputWrapper.handle_message_chat_completion(\n",
    "                            response)\n",
    "                        frame_text = llm_result['content'][len(total_response):]\n",
    "                    else:\n",
    "                        llm_result = CustomOutputWrapper.handle_message_text_completion(\n",
    "                            response)\n",
    "                        frame_text = llm_result[len(total_response):]\n",
    "                    yield frame_text\n",
    "\n",
    "                    if self.agent_type == AgentType.Messages:\n",
    "                        total_response = llm_result['content']\n",
    "                    else:\n",
    "                        total_response = llm_result\n",
    "                else:\n",
    "                    err_msg = 'Error Request id: %s, Code: %d, status: %s, message: %s' % (\n",
    "                        response.request_id, response.status_code, response.code,\n",
    "                        response.message)\n",
    "                    print(err_msg)\n",
    "                    raise RuntimeError(err_msg)\n",
    "\n",
    "    from modelscope_agent.llm import dashscope_llm\n",
    "    dashscope_llm.DashScopeLLM = my_DashScopeLLM\n",
    "\n",
    "fix_1() # 如果不需要查看模型的input和output，可以不用执行这个cell"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ec5665513daf6d22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from modelscope_agent.tools import Tool\n",
    "from modelscope_agent.llm import LLMFactory\n",
    "from modelscope_agent.prompt import MSPromptGenerator, MrklPromptGenerator\n",
    "from modelscope_agent.output_parser import MsOutputParser, MRKLOutputParser\n",
    "from modelscope.utils.config import Config\n",
    "from modelscope_agent.agent import AgentExecutor\n",
    "\n",
    "model_name = 'modelscope-agent'\n",
    "model_cfg = {\n",
    "    \"qwen-max\": {\n",
    "        \"type\": \"dashscope\",\n",
    "        \"model\": \"qwen-max\",\n",
    "        \"generate_cfg\": {\n",
    "            \"use_raw_prompt\": True,\n",
    "            \"top_p\": 0.8,\n",
    "            \"debug\": False\n",
    "        }\n",
    "    },\n",
    "    \"modelscope-agent\": {\n",
    "        \"type\": \"dashscope\",\n",
    "        \"model\": \"modelscope-agent-llm-v1\",\n",
    "        \"generate_cfg\": {\n",
    "            \"use_raw_prompt\": True,\n",
    "            \"top_p\": 0.8,\n",
    "            \"seed\": 666,\n",
    "            \"debug\": True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "llm = LLMFactory.build_llm(model_name, model_cfg)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6fa67f0021c05bdb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# prompt_generator = MrklPromptGenerator(llm=llm)\n",
    "# output_parser = MRKLOutputParser()\n",
    "\n",
    "prompt_generator = MSPromptGenerator()\n",
    "output_parser = MsOutputParser()\n",
    "\n",
    "from modelscope_agent.tools import Tool\n",
    "\n",
    "class AliyunRenewInstanceTool(Tool):\n",
    "    description = '续费一台包年包月ECS实例'\n",
    "    name = 'RenewInstance'\n",
    "    parameters: list = [{\n",
    "        'name': 'instance_id',\n",
    "        'description': 'ECS实例ID',\n",
    "        'required': True\n",
    "    },\n",
    "        {\n",
    "            'name': 'period',\n",
    "            'description': '续费时长以月为单位',\n",
    "            'required': True\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    def _local_call(self, *args, **kwargs):\n",
    "        instance_id = kwargs['instance_id']\n",
    "        period = kwargs['period']\n",
    "        return {'result': f'成功为{instance_id}续费，续费时长{period}月'}\n",
    "\n",
    "\n",
    "additional_tool_list = {\n",
    "    'RenewInstance': AliyunRenewInstanceTool()\n",
    "}\n",
    "\n",
    "agent = AgentExecutor(llm, additional_tool_list=additional_tool_list, tool_retrieval=False,\n",
    "                      prompt_generator=prompt_generator, output_parser=output_parser)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ba8d856e4d435135"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 重置对话，清空对话历史\n",
    "agent.reset()\n",
    "available_tool_list = ['RenewInstance']\n",
    "agent.set_available_tools(available_tool_list)\n",
    "agent.run(\"请帮我续费一台ECS实例，实例id是：i-rj90a7e840y5cde，续费时长10个月\", remote=False, print_info=True)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "fd9e2d7205344908"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(prompts[0])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "848a11d80f4362d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(prompts[1])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "16cf6c9e0c518368"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(outputs[0])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "27afd9f150b629f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(outputs[1])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "bd804553cc4220c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "txt = \"\"\"\n",
    "{\n",
    "   \"api_name\": \"RenewInstance\",\n",
    "   \"url\": \"https://ecs.aliyuncs.com/renewinstance\",\n",
    "   \"parameters\": {\n",
    "      \"instance_id\": \"i-rj90a7e840y5cde\",\n",
    "      \"period\": 10\n",
    "   }\n",
    "}\n",
    "\"\"\"\n",
    "data = json.loads(txt)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8e5a86268d97a375"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5797addb7e0ec342"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2a782e05a4e39c8b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
